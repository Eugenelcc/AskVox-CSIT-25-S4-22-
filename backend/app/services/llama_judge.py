from __future__ import annotations

import asyncio
import json
import os
import time
from typing import Any

import httpx
from fastapi import HTTPException

# RunPod Serverless (job-mode) envs (same naming as other modules)
RUNPOD_API_KEY = os.getenv("RUNPOD_API_KEY", "").strip()
RUNPOD_AUTH_HEADER = os.getenv("RUNPOD_AUTH_HEADER", "Authorization").strip()
RUNPOD_RUN_ENDPOINT = os.getenv("RUNPOD_RUN_ENDPOINT", "").strip()
RUNPOD_STATUS_ENDPOINT = os.getenv("RUNPOD_STATUS_ENDPOINT", "").strip()
RUNPOD_MAX_WAIT_SEC = float(os.getenv("RUNPOD_MAX_WAIT_SEC", "600"))
RUNPOD_POLL_INTERVAL_SEC = float(os.getenv("RUNPOD_POLL_INTERVAL_SEC", "1.5"))

# Optional direct OpenAI-compatible or custom endpoint
LLAMA_RUNPOD_URL = os.getenv("LLAMA_RUNPOD_URL", "").strip().rstrip("/")
LLAMA_MODEL_NAME = os.getenv("LLAMA_MODEL_NAME", "meta-llama/Llama-3-70b-instruct")

# Guardrails for cost/latency
MAX_JUDGE_TEXT_CHARS = int(os.getenv("AI_JUDGE_MAX_CHARS", "12000"))
JUDGE_TIMEOUT_SEC = float(os.getenv("AI_JUDGE_TIMEOUT_SEC", "120"))


def _extract_first_json_object(text: str) -> dict[str, Any]:
    s = (text or "").strip()
    start = s.find("{")
    end = s.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("no_json_object")
    candidate = s[start : end + 1]
    return json.loads(candidate)


def _build_judge_prompt(user_text: str) -> str:
    # Keep prompt small and deterministic. We ask for *JSON only*.
    return (
        "SYSTEM: You are a strict AI-text judge. Your task is to estimate the likelihood (0-100) that the USER text was generated by an AI language model. "
        "This is probabilistic, not proof. You MUST return ONLY valid JSON with these keys: "
        "ai_percentage (integer 0-100), rationale (string, max 2 sentences, no newlines).\n"
        "SYSTEM: Do not include markdown, code fences, or any extra keys.\n"
        "USER: <<<BEGIN TEXT>>>\n"
        f"{user_text}\n"
        "<<<END TEXT>>>\n"
        "ASSISTANT:"
    )


async def _runpod_job(prompt: str) -> str:
    if not RUNPOD_RUN_ENDPOINT:
        raise HTTPException(status_code=500, detail="RUNPOD_RUN_ENDPOINT is not configured")
    if not RUNPOD_API_KEY:
        raise HTTPException(status_code=500, detail="RUNPOD_API_KEY is missing")

    headers = {RUNPOD_AUTH_HEADER or "Authorization": f"Bearer {RUNPOD_API_KEY}"}
    payload = {"input": {"prompt": prompt, "stop": ["<|eot_id|>"]}}

    timeout = httpx.Timeout(connect=10.0, read=30.0, write=10.0, pool=10.0)
    try:
        async with httpx.AsyncClient(timeout=timeout) as client:
            run_resp = await client.post(RUNPOD_RUN_ENDPOINT, json=payload, headers=headers)
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"RunPod run request failed: {e}")

    if run_resp.status_code >= 400:
        raise HTTPException(status_code=502, detail=f"RunPod /run returned {run_resp.status_code}: {run_resp.text[:200]}")

    try:
        run_data = run_resp.json()
    except Exception:
        raise HTTPException(status_code=502, detail="Invalid JSON from RunPod /run")

    job_id = run_data.get("id") or run_data.get("jobId") or run_data.get("job_id")
    if not job_id:
        immediate_output = (run_data.get("output") or {}).get("response") or run_data.get("response")
        if immediate_output:
            return str(immediate_output).strip()
        raise HTTPException(status_code=502, detail="RunPod /run response missing job id")

    status_base = RUNPOD_STATUS_ENDPOINT.strip() if RUNPOD_STATUS_ENDPOINT else ""
    if not status_base:
        if RUNPOD_RUN_ENDPOINT.endswith("/run"):
            status_base = RUNPOD_RUN_ENDPOINT[: -len("/run")] + "/status"
        else:
            status_base = RUNPOD_RUN_ENDPOINT.rstrip("/") + "/status"

    t0 = time.perf_counter()
    last_status = ""
    status_timeout = httpx.Timeout(connect=10.0, read=20.0, write=10.0, pool=10.0)

    while (time.perf_counter() - t0) < RUNPOD_MAX_WAIT_SEC:
        url = f"{status_base}/{job_id}"
        try:
            async with httpx.AsyncClient(timeout=status_timeout) as client:
                st_resp = await client.get(url, headers=headers)
        except httpx.RequestError as e:
            last_status = f"request_error: {e}"
            await asyncio.sleep(RUNPOD_POLL_INTERVAL_SEC)
            continue

        if st_resp.status_code >= 400:
            last_status = f"http_{st_resp.status_code}"
            await asyncio.sleep(RUNPOD_POLL_INTERVAL_SEC)
            continue

        try:
            st_data = st_resp.json()
        except Exception:
            last_status = "bad_json"
            await asyncio.sleep(RUNPOD_POLL_INTERVAL_SEC)
            continue

        status = (st_data.get("status") or st_data.get("state") or "").upper()
        last_status = status or last_status

        if status == "COMPLETED":
            out = st_data.get("output") or {}
            if isinstance(out, dict):
                ans = out.get("response") or out.get("answer") or out.get("reply")
                if ans:
                    return str(ans).strip()
            if isinstance(out, str) and out:
                return out.strip()
            ans2 = st_data.get("response") or st_data.get("answer") or st_data.get("reply")
            if ans2:
                return str(ans2).strip()
            raise HTTPException(status_code=502, detail="RunPod completed but no output")

        if status in {"FAILED", "ERROR", "CANCELLED"}:
            raise HTTPException(status_code=502, detail=f"RunPod job {status}")

        await asyncio.sleep(RUNPOD_POLL_INTERVAL_SEC)

    raise HTTPException(status_code=504, detail=f"RunPod job timed out (last_status={last_status})")


async def _direct_llama_chat(prompt: str) -> str:
    if not LLAMA_RUNPOD_URL:
        raise HTTPException(status_code=500, detail="LLAMA_RUNPOD_URL is not configured")

    headers: dict[str, str] = {"Content-Type": "application/json"}
    if RUNPOD_API_KEY:
        headers["X-API-Key"] = RUNPOD_API_KEY

    # Common OpenAI-compatible endpoint
    if "/v1/chat/completions" in LLAMA_RUNPOD_URL.lower():
        payload = {
            "model": LLAMA_MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Return ONLY valid JSON."},
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.0,
            "max_tokens": 220,
            "stream": False,
        }
    else:
        payload = {"input": {"prompt": prompt}}

    try:
        async with httpx.AsyncClient(timeout=JUDGE_TIMEOUT_SEC) as client:
            res = await client.post(LLAMA_RUNPOD_URL, headers=headers, json=payload)
        res.raise_for_status()
        data = res.json()
    except httpx.RequestError as e:
        raise HTTPException(status_code=502, detail=f"LLaMA request failed: {e}")
    except Exception:
        raise HTTPException(status_code=502, detail="Invalid response from LLaMA endpoint")

    # OpenAI-compatible
    if isinstance(data, dict) and "choices" in data:
        try:
            return str(data["choices"][0]["message"]["content"]).strip()
        except Exception:
            return json.dumps(data)[:1000]

    # RunPod-style
    if isinstance(data, dict):
        out = data.get("output")
        if isinstance(out, dict) and out.get("response"):
            return str(out.get("response")).strip()
        if isinstance(out, str) and out:
            return out.strip()
        if data.get("response"):
            return str(data.get("response")).strip()

    return json.dumps(data)[:1000]


async def judge_ai_text(text: str) -> dict[str, Any]:
    t = (text or "").strip()
    if not t:
        raise HTTPException(status_code=400, detail="text is required")

    if len(t) > MAX_JUDGE_TEXT_CHARS:
        raise HTTPException(
            status_code=413,
            detail=f"Text too long for AI judge (max {MAX_JUDGE_TEXT_CHARS} chars). Upload a file or shorten the input.",
        )

    prompt = _build_judge_prompt(t)

    # Prefer job-mode when configured
    if RUNPOD_RUN_ENDPOINT:
        raw = await _runpod_job(prompt)
    else:
        raw = await _direct_llama_chat(prompt)

    try:
        parsed = _extract_first_json_object(raw)
    except Exception:
        raise HTTPException(
            status_code=502,
            detail={
                "error": "Judge model did not return valid JSON",
                "raw_preview": raw[:600],
            },
        )

    ai_pct = parsed.get("ai_percentage")
    rationale = parsed.get("rationale")

    if not isinstance(ai_pct, int):
        # sometimes models return numeric string
        if isinstance(ai_pct, str) and ai_pct.strip().isdigit():
            ai_pct = int(ai_pct.strip())
        else:
            raise HTTPException(status_code=502, detail={"error": "Missing ai_percentage", "raw": parsed})

    if not isinstance(rationale, str):
        rationale = None

    ai_pct = max(0, min(100, int(ai_pct)))

    return {
        "ai_percentage": ai_pct,
        "human_percentage": 100 - ai_pct,
        "rationale": (rationale or "").strip() or None,
    }
